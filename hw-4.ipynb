{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a22003",
   "metadata": {},
   "source": [
    "# Reading and Writing Electronic Text\n",
    "Spring 2024\n",
    "\n",
    "Myrah Sarwar\n",
    "\n",
    "***\n",
    "\n",
    "## Assignment 4\n",
    "### Digital cut-up revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36940b5c",
   "metadata": {},
   "source": [
    "#### Text sources:\n",
    "- _Manâ€™s Search for Meaning_, Victor Frankl\n",
    "- _Sense of Nonsense_ audio transcript, Alan Watts\n",
    "- _The Hitchhiker's Guide to the Galaxy_, Douglas Adams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7a124",
   "metadata": {},
   "source": [
    "Load text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "b6ef66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "frankl_txt = open(\"frankl.txt\").read()\n",
    "watts_txt = open(\"watts1.txt\").read()\n",
    "hitchhiker_txt = open(\"hitchhiker.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c49c3e",
   "metadata": {},
   "source": [
    "Printing all text files individually here in full (for my own reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "6725ffd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By declaring that man is responsible and must actualize the potential\n",
      "meaning of his life, I wish to stress that the true meaning of life is\n",
      "to be discovered in the world rather than within man or his own\n",
      "psyche, as though it were a closed system. I have termed this\n",
      "constitutive characteristic \"the self-transcendence of human\n",
      "existence.\" It denotes the fact that being human always points, and is\n",
      "directed, to something or someone, other than oneself--be it a meaning\n",
      "to fulfill or another human being to encounter. The more one forgets\n",
      "himself--by giving himself to a cause to serve or another person to\n",
      "love--the more human he is and the more he actualizes himself. What is\n",
      "called self-actualization is not an attainable aim at all, for the\n",
      "simple reason that the more one would strive for it, the more he would\n",
      "miss it. In other words, self-actualization is possible only as a\n",
      "side-effect of self-transcendence. \n",
      "\n",
      "But it seems that only in moments of unusual insight and illumination\n",
      "that we get the point of this, and find that thus the true meaning of\n",
      "life is no meaning, that its purpose is no purpose and that its sense\n",
      "is nonsense. Still we want to use about it the word significant.\n",
      "Significant nonsense? Yes, nonsense, that is not just chaos, that is\n",
      "not just blathering balderdash, but that has in it rhythm, fascinating\n",
      "complexity, a kind of artistry. It is in this kind of meaninglessness\n",
      "that we get the profoundest meaning. \n",
      "\n",
      "This planet has - or rather had - a problem, which was this: most of\n",
      "the people living on it were unhappy for pretty much of the time. Many\n",
      "solutions were suggested for this problem, but most of these were\n",
      "largely concerned with the movement of small green pieces of paper,\n",
      "which was odd because on the whole it wasn't the small green pieces of\n",
      "paper that were unhappy.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(frankl_txt), \"\\n\")\n",
    "print(textwrap.fill(watts_txt), \"\\n\")\n",
    "print(textwrap.fill(hitchhiker_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f37c8",
   "metadata": {},
   "source": [
    "Combining all text files so it's easier to extract from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "7b192e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_txt = frankl_txt + \" \" + watts_txt + \" \" + hitchhiker_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb591121",
   "metadata": {},
   "source": [
    "Set up spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4871db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install -c conda-forge -y --prefix {sys.prefix} spacy\n",
    "!{sys.executable} -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "484e7ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5724d1",
   "metadata": {},
   "source": [
    "Separate sentences/words and parts of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5b85c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = nlp(combined_txt)\n",
    "\n",
    "sentences = list(combined.sents)\n",
    "words = [w for w in list(combined) if w.is_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "97120275",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_single = [w for w in words if w.tag_ == 'NN']\n",
    "noun_plural = [w for w in words if w.tag_ == 'NNS']\n",
    "verbs_base = [w for w in words if w.tag_ == 'VB']\n",
    "verbs_present = [w for w in words if w.tag_ == 'VBG']\n",
    "verbs_past = [w for w in words if w.tag_ == 'VBN']\n",
    "adj = [w for w in words if w.tag_ == 'JJ']\n",
    "adjs = [w for w in words if w.tag_ == 'JJS']\n",
    "adv = [w for w in words if w.tag_ == 'RB']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1fd508",
   "metadata": {},
   "source": [
    "Choose a random sentence and randomly replace words with appropriate parts of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "d338c9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "It is in this kind of meaninglessness that we get the profoundest meaning.\n",
      "\n",
      "Modified:\n",
      "It is in this complexity of possible that we get the most psyche. \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sentence_base = random.choice(sentences)\n",
    "sentence_base_words = []\n",
    "\n",
    "for word in sentence_base:\n",
    "    if word.tag_ == 'NN' and word.text.islower():\n",
    "        new_word = random.choice(noun_single).text\n",
    "        sentence_base_words.append(new_word)\n",
    "    elif word.tag_ == 'NNS' and word.text.islower():\n",
    "        new_word = random.choice(noun_plural).text\n",
    "        sentence_base_words.append(new_word)  \n",
    "    elif word.tag_ == 'VB' and word.text.islower():\n",
    "        new_word = random.choice(verbs_base).text\n",
    "        sentence_base_words.append(new_word)\n",
    "    elif word.tag_ == 'VBG' and word.text.islower():\n",
    "        new_word = random.choice(verbs_present).text\n",
    "        sentence_base_words.append(new_word)\n",
    "    elif word.tag_ == 'VBN' and word.text.islower():\n",
    "        new_word = random.choice(verbs_past).text\n",
    "        sentence_base_words.append(new_word) \n",
    "    elif word.tag_ == 'JJ' and word.text.islower():\n",
    "        new_word = random.choice(adj).text\n",
    "        sentence_base_words.append(new_word)\n",
    "    elif word.tag_ == 'JJS' and word.text.islower():\n",
    "        new_word = random.choice(adjs).text\n",
    "        sentence_base_words.append(new_word) \n",
    "    elif word.tag_ == 'RB' and word.text.islower():\n",
    "        new_word = random.choice(adv).text\n",
    "        sentence_base_words.append(new_word)\n",
    "    else:\n",
    "        sentence_base_words.append(word.text)\n",
    "    sentence_base_words.append(word.whitespace_)\n",
    "\n",
    "print(\"Original:\")\n",
    "print(sentence_base)\n",
    "print()\n",
    "print(\"Modified:\")\n",
    "print(''.join(sentence_base_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946b98a",
   "metadata": {},
   "source": [
    "Didn't really like how this turned out, so trying again by separating by subject and object instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "654e165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in list(st)]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "11e53115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that man',\n",
       " 'I',\n",
       " 'the true meaning of life',\n",
       " 'it',\n",
       " 'I',\n",
       " 'It',\n",
       " 'it',\n",
       " 'The more one',\n",
       " 'he',\n",
       " 'he',\n",
       " 'What',\n",
       " 'self-actualization',\n",
       " 'the more one',\n",
       " 'he',\n",
       " 'self-actualization',\n",
       " 'it',\n",
       " 'we',\n",
       " 'the true meaning of life',\n",
       " 'its purpose',\n",
       " 'its sense',\n",
       " 'we',\n",
       " 'that',\n",
       " 'that',\n",
       " 'that',\n",
       " 'It',\n",
       " 'we',\n",
       " 'This planet',\n",
       " 'which',\n",
       " 'most of the people living on it',\n",
       " 'Many solutions',\n",
       " 'most of these',\n",
       " 'which',\n",
       " 'it',\n",
       " 'that']"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects = []\n",
    "for word in combined:\n",
    "    if word.dep_ in ('nsubj', 'nsubjpass'):\n",
    "        subjects.append(flatten_subtree(word.subtree))\n",
    "        \n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "fdec001e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the potential meaning of his life',\n",
       " 'this constitutive characteristic \"',\n",
       " 'the fact that being human always points',\n",
       " 'himself',\n",
       " 'himself',\n",
       " 'himself',\n",
       " 'it',\n",
       " 'the point of this',\n",
       " 'the word significant',\n",
       " 'balderdash',\n",
       " 'the profoundest meaning',\n",
       " 'a problem, which was this']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects = []\n",
    "for word in combined:\n",
    "    if word.dep_ in ('dobj', 'iobj'):\n",
    "        objects.append(flatten_subtree(word.subtree))\n",
    "        \n",
    "objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b65f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sentence_base = random.choice(sentences)\n",
    "SOV = []\n",
    "\n",
    "for word in sentence_base:\n",
    "    if word.dep_ in ('dobj', 'iobj'):\n",
    "        new_words = random.choice(objects)\n",
    "        SOV.append(new_words)\n",
    "    if word.dep_ in ('nsubj', 'nsubjpass'):\n",
    "        new_words = random.choice(subjects)\n",
    "        SOV.append(new_words)  \n",
    "    else:\n",
    "        SOV.append(word.text)\n",
    "        SOV.append(word.whitespace_)\n",
    "        \n",
    "print(sentence_base)\n",
    "print()\n",
    "print(''.join(SOV))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432c63b",
   "metadata": {},
   "source": [
    "That code is not quite working either so I am giving up on that and, this time, trying by using word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f2594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector\n",
    "\n",
    "!curl -L -O https://raw.githubusercontent.com/aparrish/wordfreq-en-25000/main/wordfreq-en-25000-log.json\n",
    "    \n",
    "import json\n",
    "prob_lookup = dict(json.load(open(\"./wordfreq-en-25000-log.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "13eca048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpleneighbors import SimpleNeighbors\n",
    "\n",
    "lookup = SimpleNeighbors(300)\n",
    "for word in prob_lookup.keys():\n",
    "    if nlp.vocab[word].has_vector:\n",
    "        lookup.add_one(word, vec(word))\n",
    "lookup.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12867ec",
   "metadata": {},
   "source": [
    "Printing 5 times (first being the original) with each one becoming more complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "54bfc463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Yes, nonsense, that is not just chaos, that is not just blathering balderdash, but that has in it rhythm, fascinating complexity, a kind of artistry.\n",
      "\n",
      "Variations:\n",
      "Yes, nonsense, that is not just chaos, that is not just blathering\n",
      "beard, but that has in it rhythm, fascinating complexity, a really of\n",
      "contemporary.\n",
      "\n",
      "\n",
      "Yes, rhetoric, that is not just woes, that is not just blathering\n",
      "finch, but that has in it motif, persuasive turbulence, a strangely of\n",
      "theatrical.\n",
      "\n",
      "\n",
      "Yes, foolish, that is not just tragedies, that is not just blathering\n",
      "footed, but that has in it fling, enchanting preferential, a terrible\n",
      "of figurative.\n",
      "\n",
      "\n",
      "Yes, overtly, that is not just elves, that is not just blathering\n",
      "turtle, but that has in it knuckles, sighting transformations, a\n",
      "debatable of vernacular.\n",
      "\n",
      "\n",
      "Yes, perspective, that is not just miracles, that is not just\n",
      "blathering blond, but that has in it axis, aspiring misunderstand, a\n",
      "menace of international.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_base = random.choice(sentences)\n",
    "sentence_synonyms = []\n",
    "distance = 3;\n",
    "\n",
    "print(\"Original:\")\n",
    "print(sentence_base)\n",
    "print()\n",
    "print(\"Variations:\")\n",
    "\n",
    "for repeat in range(5):\n",
    "    for word in sentence_base:\n",
    "        if word.is_alpha and word.pos_ in ('NOUN', 'ADJ'):\n",
    "            new_word = random.choice(lookup.nearest(word.vector, distance))\n",
    "            sentence_synonyms.append(new_word)  \n",
    "        else:\n",
    "            sentence_synonyms.append(word.text)\n",
    "        sentence_synonyms.append(word.whitespace_)\n",
    "    sentence_updated = ''.join(sentence_synonyms) \n",
    "    print(textwrap.fill(sentence_updated) + \"\\n\\n\")\n",
    "    sentence_synonyms = []\n",
    "    distance *= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859ee96",
   "metadata": {},
   "source": [
    "**Original:**\n",
    ">\n",
    ">_Significant nonsense?_\n",
    ">\n",
    "**Variations:**\n",
    ">\n",
    ">_insignificant nonsense?_\n",
    ">\n",
    ">_significance despise?_\n",
    ">\n",
    ">_suggestive honesty?_\n",
    ">\n",
    ">_substantially kind?_\n",
    ">\n",
    ">_dissemination skeptical?_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
