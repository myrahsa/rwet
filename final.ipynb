{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d72d4d9-3cd3-4654-9f5e-dafa7a0c69e6",
   "metadata": {},
   "source": [
    "# Reading and Writing Electronic Text\n",
    "Spring 2024\n",
    "\n",
    "Myrah Sarwar\n",
    "\n",
    "## Final Assignment\n",
    "#### [Link to documentation blog post](https://myrahsarwar.com/_rwet_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc7d07-3e3e-48e7-a7e8-8666eb8299c9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406cb65-e068-4f88-aa89-07caad0243ef",
   "metadata": {},
   "source": [
    "#### Installing things I need to install!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dcbd00-86fd-47c9-94cb-dfdc38ee4aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --prefix {sys.prefix} -y -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1255b-a8b2-4b3a-840a-90b40ee552b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5ab5b-130c-40bf-9dac-25afbf482e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e60837c1-9fdb-4a22-a3ec-a31f2ec73801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9cca277e-ad6a-4760-8e90-8670615bc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3476063-79f6-4b56-9ef1-0dfa22491e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7c000-053a-4cc3-ac93-7b8b0ffa5a7e",
   "metadata": {},
   "source": [
    "#### Combining all of my text files by category\n",
    "\n",
    "This will give me the flexibility to work with specific areas only if needed later on. Right now, my notes are separated based on the main folder they were in in my notes application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fd7b4-a66c-4aa1-83bd-d3cd2ff9242b",
   "metadata": {},
   "source": [
    "**all** of my files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74d315b6-631e-4612-bd7b-44a93af95080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation complete. Check 'all-text.txt' for the result.\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"final_txt/itp.txt\", \"final_txt/misc_tags.txt\",\"final_txt/notes_icloud.txt\",\"final_txt/notes_local.txt\",\"final_txt/school_apps.txt\",\"final_txt/umg.txt\",\"final_txt/usc.txt\"]\n",
    "\n",
    "with open(\"all-text.txt\", \"w\") as new_file:\n",
    "    for name in filenames:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                new_file.write(line)\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "print(\"Concatenation complete. Check 'all-text.txt' for the result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e2095-ce03-45c3-a816-d815479e9b65",
   "metadata": {},
   "source": [
    "**personal** files only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e5ef66-c9e8-4346-b621-983de2a6b753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation complete. Check 'personal-text.txt' for the result.\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"final_txt/notes_icloud.txt\",\"final_txt/notes_local.txt\"]\n",
    "\n",
    "with open(\"personal-text.txt\", \"w\") as new_file:\n",
    "    for name in filenames:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                new_file.write(line)\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "print(\"Concatenation complete. Check 'personal-text.txt' for the result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c41ac-1c30-4ab7-84fd-fdba4600a817",
   "metadata": {},
   "source": [
    "**academic + work** files only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13cbca68-f541-4a6c-9d59-4fddd3259de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation complete. Check 'acad-text.txt' for the result.\n"
     ]
    }
   ],
   "source": [
    "filenames = [\"final_txt/itp.txt\",\"final_txt/umg.txt\",\"final_txt/usc.txt\"]\n",
    "\n",
    "with open(\"acad-text.txt\", \"w\") as new_file:\n",
    "    for name in filenames:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                new_file.write(line)\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "print(\"Concatenation complete. Check 'acad-text.txt' for the result.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e446d-345e-43d7-b5a6-256f07fb86b7",
   "metadata": {},
   "source": [
    "Realizing after that I probably should have just done this with one big loop instead of three separate ones, but it's okay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80feb6e1-f567-4e8d-9e41-43551bd855e7",
   "metadata": {},
   "source": [
    "#### Now to train the model on my text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d626ec-6a90-45d2-965e-a3d0192a1008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13b28b9b-40d3-4e67-8c90-c752616c547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4898153-b270-45ed-9870-b66e98e866d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.load_dataset('text', data_files=\"all-text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a17ba1dc-51ff-4014-b380-de56c3bafb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_training_data = training_data.map(\n",
    "    lambda x: tokenizer(x['text']),\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5419b7c3-7c2b-41b6-b6bb-1f5536ea69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_training_data = tokenized_training_data.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbb8d2b7-a610-49e6-a14e-51c4cf068000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd55beba-b641-4dfe-aeea-99fd0aa00faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  train_dataset=lm_training_data['train'],\n",
    "                  args=TrainingArguments(\n",
    "                      output_dir='distilgpt2-finetune-erotic',\n",
    "                      num_train_epochs=40,\n",
    "                      do_train=True,\n",
    "                      do_eval=False,\n",
    "                  ),\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9912940-478b-4bfe-9c27-6850134697fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a539c0a-ab8c-4da2-aa6a-24d68fe71685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c629d0-745d-4e3f-b49b-c003807a4afe",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b462ee1-cae2-49bd-b3fa-510bc291b52a",
   "metadata": {},
   "source": [
    "#### Testing outputs for \"note titles\" in three ways:\n",
    "* text with a line break\n",
    "* text in all caps (since I do this often in my notes)\n",
    "* text with colon after (to make it clear it's a title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f5360-b3dc-4d59-a4ae-897073286f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generator(\"today\\n\", max_length=100, truncation=True)[0]['generated_text']\n",
    "\n",
    "generated_text_corrected = generated_text.replace(\"-\", \"\\n-\").replace(\":\", \":\\n\").replace(\"*\", \"\\n*\")\n",
    "\n",
    "print(generated_text_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da22e8-1354-4212-837c-be06c5f54716",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generator(\"THINGS TO DO\", max_length=50, truncation=True)[0]['generated_text']\n",
    "\n",
    "generated_text_corrected = generated_text.replace(\"-\", \"\\n-\").replace(\":\", \":\\n\").replace(\"*\", \"\\n*\")\n",
    "\n",
    "print(generated_text_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d6fea-0e75-43db-8d8c-02efc3b29749",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generator(\"food I have:\", max_length=40, truncation=True)[0]['generated_text']\n",
    "\n",
    "generated_text_corrected = generated_text.replace(\"-\", \"\\n-\").replace(\":\", \":\\n\").replace(\"*\", \"\\n*\")\n",
    "\n",
    "print(generated_text_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6797f14-39ce-4d10-97c7-c8067c8988a9",
   "metadata": {},
   "source": [
    "**Some results (@ 20 epochs):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b27c1a-c305-4e3b-a624-c77e86aa97b2",
   "metadata": {},
   "source": [
    "~~~\n",
    "my feelings today:\n",
    "\n",
    " are these emotions justified? are these unspoken few feelings really justified or do they just make me feel worthless and worthless instead of understanding and being humanlikeI feel strongly about the existence of the universe because we are all created equal, as individuals, but the term has become too misnomer and misconstrued and misconstruedMy comfort level is low because I don’t know why I’m so misinformed or why I think I have such a big interest in things like the things I care aboutI am so afraid to let too much into things like these other times. My life is too filled with me but this is why I feel so angry a lot for the dayIt’s always hard to change. life has to be joyful and feel so empty and feel empty every time so I try to feel like I need to be myself anymore I realize that everything is fine When I feel like I feel sad, I feel like it goes away\n",
    "\n",
    "----\n",
    "\n",
    "to do list\n",
    "\n",
    "- check out list of tasks    \n",
    "- find tasks through list of tasks, you want to describe the tasks\n",
    "- tasks    \n",
    "- you can create new ones\n",
    "- remember to put the output of each task in a separate file so it is easier to find and find things that you are looking for    \n",
    "- you can create a list of tasks of any given task (e.g., go up to 2 different managers, right?)\n",
    "\n",
    "----\n",
    "\n",
    "list\n",
    "\n",
    "- [x] remi mac n cheese\n",
    "- [x] salad tin (vegetables)\n",
    "- [x] sour cream\n",
    "- [x] cheese\n",
    "- [x] paprika\n",
    "- [x] saltine crackers\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9e56de-2d2e-4dbb-8f9a-d0bc5b92eff5",
   "metadata": {},
   "source": [
    "**@ 40 epochs:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8c6b5-49c2-41c2-9c1c-ccb3572af15f",
   "metadata": {},
   "source": [
    "~~~\n",
    "spotify playlist:\n",
    "    \n",
    "- mix of ambient / rock 'n stuff or whatever\n",
    "\n",
    "---\n",
    "\n",
    "places:\n",
    "\n",
    "- alison mcdonalds pizza place(??)\n",
    "- bovard, the tailor (undiscovered)wilshire vintage (thanksgiving)things to consider things to keep in mind\n",
    "- vegetarian options good coffee places if\n",
    "\n",
    "---\n",
    "\n",
    "STUDY PLAN:\n",
    "\n",
    "- slow down, take steps\n",
    "- slowly take a few steps\n",
    "- slowly get used to confidence again and be mindful\n",
    "- Don’t take pr too long \n",
    "- you will only feel certain things during practice and you will feel happy even just a few moments. Today is one of many times so try to focus on your mundane tasks. Today, you might not find things that are important so try to feel good/peaceful enough so you can focus on your task list\n",
    "\n",
    "---\n",
    "\n",
    "list of my accomplishments:\n",
    "\n",
    "What did I do? What did I enjoy most? Are accomplishments I failed to accomplish?\n",
    "\n",
    "---\n",
    "\n",
    "list of my accomplishments:\n",
    "    \n",
    "- 25k followers    \n",
    "- 478 followers\n",
    "\n",
    "---\n",
    "\n",
    "list of my accomplishments:\n",
    " \n",
    "- completed homework in May of this year\n",
    "\n",
    "---\n",
    "\n",
    "stuff to do\n",
    "\n",
    "- don’t act too full of pretentious shit\n",
    "\n",
    "---\n",
    "\n",
    "food I have\n",
    "\n",
    "- instant coffee\n",
    "- instant coffee \n",
    "- great value of energy (which I feel california consumes)\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbe377b-30d7-4913-aad4-d7b5a9db2192",
   "metadata": {},
   "source": [
    "***\n",
    "Using \"I\" and \"It\" only (which I manually chose) as the starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28c6c0-195d-4926-83a8-1c09f0f05f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generator(\"I am\")[0]['generated_text']\n",
    "\n",
    "generated_text_corrected = generated_text.replace(\"-\", \"\\n-\").replace(\":\", \":\\n\").replace(\"*\", \"\\n*\")\n",
    "\n",
    "print(generated_text_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffcbd7-5b29-413f-911e-d10bd2a22d12",
   "metadata": {},
   "source": [
    "**Example outputs:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3c4ae-7241-4f06-b3ec-7e8c795f26f5",
   "metadata": {},
   "source": [
    "~~~\n",
    "I’m sorry for you. I was just getting close enough to escaping or something. I wanted to die.Then\n",
    "\n",
    "----\n",
    "\n",
    "It’s a little hard to keep track lol   <3\n",
    "\n",
    "----\n",
    "\n",
    "It is a company. I worked at supermarkets for more than a million years\n",
    "\n",
    "----\n",
    "\n",
    "I am convinced that there is life i have, and that is the way life can be\n",
    "\n",
    "----\n",
    "\n",
    "I’ve got a plan for my life that is completely different from what was happening there and I think that’s the direction i was supposed to go in but it turned out that i\n",
    "\n",
    "---\n",
    "\n",
    "I am the most important person on this planet. and this is because i feel this way because of all the things that are causing me just as much pain as the pain i feel myself living. I would personally rather live without me now\n",
    "\n",
    "---\n",
    "\n",
    "I am the only one who is manipulating the world.  I have the power to change things for the betterment of the your life, so i can’t change either. hope this will be my last class\n",
    "\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021cbb7-5390-4a49-9846-acf499907f7d",
   "metadata": {},
   "source": [
    "***\n",
    "Looking at the above results altogether makes me think this could be interesting to try to generate multiple lines to make actual poetry (which is what I should have done anyway for the class since that is the point), so let's try it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecf98ed-9dbb-4fc4-89e1-5839eb55b324",
   "metadata": {},
   "source": [
    "4 lines, each starting with the word \"I\", decreasing the amount of words with each line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee0f89-ee94-428b-98d0-ce2847de9612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length = 15\n",
    "for i in range (4):\n",
    "    generated_text = generator(\"I\", max_length=length, truncation=True)[0]['generated_text']\n",
    "    print(generated_text + \"\\n\")\n",
    "    length -= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03fe8d6-d0ea-41d8-81ec-7df2debc0ca0",
   "metadata": {},
   "source": [
    "Some outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a1f44-de1f-4ccd-8653-b53b5cbc080e",
   "metadata": {},
   "source": [
    "~~~\n",
    "I’ve spent time in similar communities as well as places like reddit\n",
    "\n",
    "I’d like to talk to her again, but\n",
    "\n",
    "I’d rather stay in the shadows\n",
    "\n",
    "I will always have a story\n",
    "\n",
    "---\n",
    "\n",
    "I’ve tried so hard to keep quiet I can’\n",
    "\n",
    "I just need more motivation to be authentic as a personI\n",
    "\n",
    "I - I feel like I alone in life\n",
    "\n",
    "I\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36ca5e-d2d4-4d41-8e11-b172619585d2",
   "metadata": {},
   "source": [
    "Again but starting each line with random preposition or pronoun. I'm getting rid of the line length decreasing because it's not contributing much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "ba50f984-4f21-4c60-a7ef-df2fa7fdcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepositions = [\"about\", \"above\", \"across\", \"after\", \"against\", \"along\", \"among\", \"around\", \"at\", \"before\", \n",
    "                \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"by\", \"down\", \"during\", \"except\", \n",
    "                \"for\", \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"of\", \"off\", \"on\", \"onto\", \"out\", \"outside\", \n",
    "                \"over\", \"past\", \"since\", \"through\", \"throughout\", \"to\", \"toward\", \"under\", \"underneath\", \"until\", \n",
    "                \"up\", \"upon\", \"with\", \"within\", \"without\"]\n",
    "\n",
    "pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"myself\", \"yourself\", \n",
    "            \"himself\", \"herself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\", \"mine\", \"yours\", \"his\", \"hers\", \n",
    "            \"its\", \"ours\", \"theirs\", \"this\", \"that\", \"these\", \"those\", \"who\", \"whom\", \"whose\", \"which\", \"what\", \"whatever\", \n",
    "            \"whoever\", \"whichever\", \"whomever\", \"my\", \"your\", \"our\", \"their\"]\n",
    "\n",
    "prep_pro = prepositions + pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c35c6-b936-4106-921d-1953a48d023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "length = 10\n",
    "for i in range(4):\n",
    "    starting_word = random.choice(prepositions)\n",
    "    generated_text = generator(starting_word, max_length=length)[0]['generated_text']\n",
    "    generated_text = generated_text.capitalize()\n",
    "    print(generated_text + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46a34c-6d24-47ea-8e1e-c659727e7ab9",
   "metadata": {},
   "source": [
    "Most of those outputs didn't make much sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f907ce-8c99-4488-8675-bb1687deb31d",
   "metadata": {},
   "source": [
    "***\n",
    "#### Markov chains!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75bec7-9d5d-4487-8467-7bf17b02b945",
   "metadata": {},
   "source": [
    "I'm also trying with markov chains just in case those results somehow end up being better. (They were not.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d9481-2eb0-4da4-a0cd-47ab1a8c11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install markovify\n",
    "\n",
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36036d4f-54bb-4500-a982-bdc711b743ac",
   "metadata": {},
   "source": [
    "Once again separating by categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "9ea254dd-ab4d-4d29-966f-008ee32181f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = open(\"all-text.txt\").read()\n",
    "all_text_generator = markovify.Text(all_text, state_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "4a29d404-4a3c-4c50-8888-be3e393fbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_text = open(\"personal-text.txt\").read()\n",
    "personal_text_generator = markovify.Text(personal_text, state_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "9b6927fc-c2e3-425a-9493-b1095e1107c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acad_text = open(\"acad-text.txt\").read()\n",
    "acad_text_generator = markovify.Text(acad_text, state_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000956c-593d-4a4d-87e6-01a82e3df837",
   "metadata": {},
   "source": [
    "These are generating sentences at random without me giving any direction. That's probably fine otherwise, but it's not what I want for this. I've also separated based on content to see if the outputs might make more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fca9a5-2cf8-40af-964b-a5bd020f5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_text_generator.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db55901-f10d-4fb7-b74c-317bd0238be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(personal_text_generator.make_short_sentence(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8c7a0-664c-4e5c-8066-6ca7908f51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acad_text_generator.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fff14-666a-4a2e-8e0c-58fd20f58afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 90\n",
    "\n",
    "for i in range(3):\n",
    "    print(all_text_generator.make_short_sentence(length))\n",
    "    print(\"\")\n",
    "    length -= 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e52e89-bb71-4a09-886a-b4e34405335f",
   "metadata": {},
   "source": [
    "This is not really working how I need it to because I want to be able to specify the first words. I think the closest I can get to this here is using the `make_sentence_with_start`, but this only picks sentences in my notes that start with the exact same words (instead of occurring anywhere)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17babf0e-16c2-4dd1-800d-e9e11824c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = all_text_generator.make_sentence_with_start(beginning=\"it\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b79d5-fbcf-404e-ada8-dd9a3c20b9e1",
   "metadata": {},
   "source": [
    "I thought maybe the language would be more consistent if I chained the sentences, but that doesn't work either because the last word of the previous sentence (usually) is too specific to occur at the start anywhere else. It would always throw me an error, like below. I also don't like that it's giving me full sentences when a good chunk of my notes are in bullet point format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "id": "814efd85-632c-4c54-b254-ff0c45b889cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParamError",
     "evalue": "`make_sentence_with_start` can't find sentence beginning with cry.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParamError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1317], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m paragraph \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m all_text_generator\u001b[38;5;241m.\u001b[39mmake_sentence() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sentences \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m all_text_generator\u001b[38;5;241m.\u001b[39mmake_sentence_with_start(beginning\u001b[38;5;241m=\u001b[39mparagraph\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      8\u001b[0m     paragraph \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sentence \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(paragraph)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/markovify/text.py:303\u001b[0m, in \u001b[0;36mText.make_sentence_with_start\u001b[0;34m(self, beginning, strict, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    300\u001b[0m err_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`make_sentence_with_start` can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find sentence beginning with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeginning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m )\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ParamError(err_msg)\n",
      "\u001b[0;31mParamError\u001b[0m: `make_sentence_with_start` can't find sentence beginning with cry."
     ]
    }
   ],
   "source": [
    "sentences = 3\n",
    "paragraph = \"\"\n",
    "\n",
    "paragraph += all_text_generator.make_sentence() + \" \"\n",
    "\n",
    "for i in range(sentences - 1):\n",
    "    sentence = all_text_generator.make_sentence_with_start(beginning=paragraph.split()[-1])\n",
    "    paragraph += sentence + \" \"\n",
    "print(paragraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
