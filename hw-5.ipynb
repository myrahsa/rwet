{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0be77bd0",
   "metadata": {},
   "source": [
    "# Reading and Writing Electronic Text\n",
    "Spring 2024\n",
    "\n",
    "Myrah Sarwar\n",
    "\n",
    "***\n",
    "\n",
    "## Assignment 5\n",
    "### Predictive model text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f7f54",
   "metadata": {},
   "source": [
    "I was interested in exploring books with unreliable narrators and wanted to try combining this with text generators. I mostly just wanted to see how noticeable the generated parts would be considering that the logic in the original is already not that credible to begin with. My guess was that it would probably not blend in naturally since the models being used are fairly small.\n",
    "\n",
    "For my source texts, I chose a couple chapters from **_Crime and Punishment_ by Fyodor Dostoevsky** and **_Life of Pi_ by Yann Martel**. \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1727f050",
   "metadata": {},
   "source": [
    "**Load text files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5adad561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“I don’t believe it, I can’t believe it!” repeated Razumihin, trying in perplexity to refute Raskolnikov’s arguments.\n",
      "\n",
      "They were by now approaching Bakaleyev’s lodgings, where Pulcheria Alexandrovna and Dounia had been expecting them a long while. Razumihin kept stopping on the way in the heat of discussion, confused and excited by the very fact that they were for the first time speaking openly about it.\n",
      "\n",
      "“Don’t believe it, then!” answered Raskolnikov, with a cold, careless smile. “You were noticing nothing as usual, but I was weighing every word.”\n",
      "\n",
      "“You are suspicious. That is why you weighed their words… h’m… certainly, I agree, Porfiry’s tone was rather strange, and still more that wretch Zametov!... You are right, there was something about him—but why? Why?”\n",
      "\n",
      "“He has changed his mind since last night.”\n",
      "\n",
      "“Quite the contrary! If they had that brainless idea, they would do their utmost to hide it, and conceal their cards, so as to catch you afterwards…. But it was all impudent and careless.”\n",
      "\n",
      "“If they had had facts—I mean, real facts—or at least grounds for suspicion, then they would certainly have tried to hide their game, in the hope of getting more (they would have made a search long ago besides). But they have no facts, not one. It is all mirage—all ambiguous. Simply a floating idea. So they try to throw me out by impudence. And perhaps, he was irritated at having no facts, and blurted it out in his vexation—or perhaps he has some plan… he seems an intelligent man. Perhaps he wanted to frighten me by pretending to know. They have a psychology of their own, brother. But it is loathsome explaining it all. Stop!”\n",
      "\n",
      "“And it’s insulting, insulting! I understand you. But… since we have spoken openly now (and it is an excellent thing that we have at last—I am glad) I will own now frankly that I noticed it in them long ago, this idea. Of course the merest hint only—an insinuation—but why an insinuation even? How dare they? What foundation have they? If only you knew how furious I have been. Think only! Simply because a poor student, unhinged by poverty and hypochondria, on the eve of a severe delirious illness (note that), suspicious, vain, proud, who has not seen a soul to speak to for six months, in rags and in boots without soles, has to face some wretched policemen and put up with their insolence; and the unexpected debt thrust under his nose, the I.O.U. presented by Tchebarov, the new paint, thirty degrees Reaumur and a stifling atmosphere, a crowd of people, the talk about the murder of a person where he had been just before, and all that on an empty stomach—he might well have a fainting fit! And that, that is what they found it all on! Damn them! I understand how annoying it is, but in your place, Rodya, I would laugh at them, or better still, spit in their ugly faces, and spit a dozen times in all directions. I’d hit out in all directions, neatly too, and so I’d put an end to it. Damn them! Don’t be downhearted. It’s a shame!”\n",
      "\n",
      "“He really has put it well, though,” Raskolnikov thought.\n",
      "\n",
      "“Damn them? But the cross-examination again, to-morrow?” he said with bitterness. “Must I really enter into explanations with them? I feel vexed as it is, that I condescended to speak to Zametov yesterday in the restaurant….”\n",
      "\n",
      "“Damn it! I will go myself to Porfiry. I will squeeze it out of him, as one of the family: he must let me know the ins and outs of it all! And as for Zametov…”\n",
      "\n",
      "“At last he sees through him!” thought Raskolnikov.\n",
      "\n",
      "“Stay!” cried Razumihin, seizing him by the shoulder again. “Stay! you were wrong. I have thought it out. You are wrong! How was that a trap? You say that the question about the workmen was a trap. But if you had done that, could you have said you had seen them painting the flat… and the workmen? On the contrary, you would have seen nothing, even if you had seen it. Who would own it against himself?”\n",
      "\n",
      "“If I had done that thing, I should certainly have said that I had seen the workmen and the flat,” Raskolnikov answered, with reluctance and obvious disgust.\n",
      "\n",
      "“But why speak against yourself?”\n",
      "\n",
      "“Because only peasants, or the most inexperienced novices deny everything flatly at examinations. If a man is ever so little developed and experienced, he will certainly try to admit all the external facts that can’t be avoided, but will seek other explanations of them, will introduce some special, unexpected turn, that will give them another significance and put them in another light. Porfiry might well reckon that I should be sure to answer so, and say I had seen them to give an air of truth, and then make some explanation.”\n",
      "\n",
      "“But he would have told you at once that the workmen could not have been there two days before, and that therefore you must have been there on the day of the murder at eight o’clock. And so he would have caught you over a detail.”\n",
      "\n",
      "“Yes, that is what he was reckoning on, that I should not have time to reflect, and should be in a hurry to make the most likely answer, and so would forget that the workmen could not have been there two days before.”\n",
      "\n",
      "“But how could you forget it?”\n",
      "\n",
      "“Nothing easier. It is in just such stupid things clever people are most easily caught. The more cunning a man is, the less he suspects that he will be caught in a simple thing. The more cunning a man is, the simpler the trap he must be caught in. Porfiry is not such a fool as you think….”\n",
      "\n",
      "“He is a knave then, if that is so!”\n",
      "\n",
      "Raskolnikov could not help laughing. But at the very moment, he was struck by the strangeness of his own frankness, and the eagerness with which he had made this explanation, though he had kept up all the preceding conversation with gloomy repulsion, obviously with a motive, from necessity.\n",
      "\n",
      "“I am getting a relish for certain aspects!” he thought to himself. But almost at the same instant he became suddenly uneasy, as though an unexpected and alarming idea had occurred to him. His uneasiness kept on increasing. They had just reached the entrance to Bakaleyev’s.\n",
      "\n",
      "“Go in alone!” said Raskolnikov suddenly. “I will be back directly.”\n",
      "\n",
      "“Where are you going? Why, we are just here.”\n",
      "\n",
      "“I can’t help it…. I will come in half an hour. Tell them.”\n",
      "\n",
      "“Say what you like, I will come with you.”\n",
      "\n",
      "“You, too, want to torture me!” he screamed, with such bitter irritation, such despair in his eyes that Razumihin’s hands dropped. He stood for some time on the steps, looking gloomily at Raskolnikov striding rapidly away in the direction of his lodging. At last, gritting his teeth and clenching his fist, he swore he would squeeze Porfiry like a lemon that very day, and went up the stairs to reassure Pulcheria Alexandrovna, who was by now alarmed at their long absence.\n",
      "\n",
      "When Raskolnikov got home, his hair was soaked with sweat and he was breathing heavily. He went rapidly up the stairs, walked into his unlocked room and at once fastened the latch. Then in senseless terror he rushed to the corner, to that hole under the paper where he had put the things; put his hand in, and for some minutes felt carefully in the hole, in every crack and fold of the paper. Finding nothing, he got up and drew a deep breath. As he was reaching the steps of Bakaleyev’s, he suddenly fancied that something, a chain, a stud or even a bit of paper in which they had been wrapped with the old woman’s handwriting on it, might somehow have slipped out and been lost in some crack, and then might suddenly turn up as unexpected, conclusive evidence against him.\n",
      "\n",
      "He stood as though lost in thought, and a strange, humiliated, half senseless smile strayed on his lips. He took his cap at last and went quietly out of the room. His ideas were all tangled. He went dreamily through the gateway.\n",
      "\n",
      "“Here he is himself,” shouted a loud voice.\n",
      "\n",
      "He raised his head.\n",
      "\n",
      "The porter was standing at the door of his little room and was pointing him out to a short man who looked like an artisan, wearing a long coat and a waistcoat, and looking at a distance remarkably like a woman. He stooped, and his head in a greasy cap hung forward. From his wrinkled flabby face he looked over fifty; his little eyes were lost in fat and they looked out grimly, sternly and discontentedly.\n",
      "\n",
      "“What is it?” Raskolnikov asked, going up to the porter.\n",
      "\n",
      "The man stole a look at him from under his brows and he looked at him attentively, deliberately; then he turned slowly and went out of the gate into the street without saying a word.\n",
      "\n",
      "“What is it?” cried Raskolnikov.\n",
      "\n",
      "“Why, he there was asking whether a student lived here, mentioned your name and whom you lodged with. I saw you coming and pointed you out and he went away. It’s funny.”\n",
      "\n",
      "The porter too seemed rather puzzled, but not much so, and after wondering for a moment he turned and went back to his room.\n",
      "\n",
      "Raskolnikov ran after the stranger, and at once caught sight of him walking along the other side of the street with the same even, deliberate step with his eyes fixed on the ground, as though in meditation. He soon overtook him, but for some time walked behind him. At last, moving on to a level with him, he looked at his face. The man noticed him at once, looked at him quickly, but dropped his eyes again; and so they walked for a minute side by side without uttering a word.\n",
      "\n",
      "“You were inquiring for me… of the porter?” Raskolnikov said at last, but in a curiously quiet voice.\n",
      "\n",
      "The man made no answer; he didn’t even look at him. Again they were both silent.\n",
      "\n",
      "“Why do you… come and ask for me… and say nothing…. What’s the meaning of it?”\n",
      "\n",
      "Raskolnikov’s voice broke and he seemed unable to articulate the words clearly.\n",
      "\n",
      "The man raised his eyes this time and turned a gloomy sinister look at Raskolnikov.\n",
      "\n",
      "“Murderer!” he said suddenly in a quiet but clear and distinct voice.\n",
      "\n",
      "Raskolnikov went on walking beside him. His legs felt suddenly weak, a cold shiver ran down his spine, and his heart seemed to stand still for a moment, then suddenly began throbbing as though it were set free. So they walked for about a hundred paces, side by side in silence.\n",
      "\n",
      "The man did not look at him.\n",
      "\n",
      "“What do you mean… what is…. Who is a murderer?” muttered Raskolnikov hardly audibly.\n",
      "\n",
      "“You are a murderer,” the man answered still more articulately and emphatically, with a smile of triumphant hatred, and again he looked straight into Raskolnikov’s pale face and stricken eyes.\n",
      "\n",
      "They had just reached the cross-roads. The man turned to the left without looking behind him. Raskolnikov remained standing, gazing after him. He saw him turn round fifty paces away and look back at him still standing there. Raskolnikov could not see clearly, but he fancied that he was again smiling the same smile of cold hatred and triumph.\n",
      "\n",
      "With slow faltering steps, with shaking knees, Raskolnikov made his way back to his little garret, feeling chilled all over. He took off his cap and put it on the table, and for ten minutes he stood without moving. Then he sank exhausted on the sofa and with a weak moan of pain he stretched himself on it. So he lay for half an hour.\n",
      "\n",
      "He thought of nothing. Some thoughts or fragments of thoughts, some images without order or coherence floated before his mind—faces of people he had seen in his childhood or met somewhere once, whom he would never have recalled, the belfry of the church at V., the billiard table in a restaurant and some officers playing billiards, the smell of cigars in some underground tobacco shop, a tavern room, a back staircase quite dark, all sloppy with dirty water and strewn with egg-shells, and the Sunday bells floating in from somewhere…. The images followed one another, whirling like a hurricane. Some of them he liked and tried to clutch at, but they faded and all the while there was an oppression within him, but it was not overwhelming, sometimes it was even pleasant…. The slight shivering still persisted, but that too was an almost pleasant sensation.\n",
      "\n",
      "He heard the hurried footsteps of Razumihin; he closed his eyes and pretended to be asleep. Razumihin opened the door and stood for some time in the doorway as though hesitating, then he stepped softly into the room and went cautiously to the sofa. Raskolnikov heard Nastasya’s whisper:\n",
      "\n",
      "“Don’t disturb him! Let him sleep. He can have his dinner later.”\n",
      "\n",
      "“Quite so,” answered Razumihin. Both withdrew carefully and closed the door. Another half-hour passed. Raskolnikov opened his eyes, turned on his back again, clasping his hands behind his head.\n",
      "\n",
      "“Who is he? Who is that man who sprang out of the earth? Where was he, what did he see? He has seen it all, that’s clear. Where was he then? And from where did he see? Why has he only now sprung out of the earth? And how could he see? Is it possible? Hm…” continued Raskolnikov, turning cold and shivering, “and the jewel case Nikolay found behind the door—was that possible? A clue? You miss an infinitesimal line and you can build it into a pyramid of evidence! A fly flew by and saw it! Is it possible?” He felt with sudden loathing how weak, how physically weak he had become. “I ought to have known it,” he thought with a bitter smile. “And how dared I, knowing myself, knowing how I should be, take up an axe and shed blood! I ought to have known beforehand…. Ah, but I did know!” he whispered in despair. At times he came to a standstill at some thought.\n",
      "\n",
      "“No, those men are not made so. The real Master to whom all is permitted storms Toulon, makes a massacre in Paris, forgets an army in Egypt, wastes half a million men in the Moscow expedition and gets off with a jest at Vilna. And altars are set up to him after his death, and so all is permitted. No, such people, it seems, are not of flesh but of bronze!”\n",
      "\n",
      "One sudden irrelevant idea almost made him laugh. Napoleon, the pyramids, Waterloo, and a wretched skinny old woman, a pawnbroker with a red trunk under her bed—it’s a nice hash for Porfiry Petrovitch to digest! How can they digest it! It’s too inartistic. “A Napoleon creep under an old woman’s bed! Ugh, how loathsome!”\n",
      "\n",
      "At moments he felt he was raving. He sank into a state of feverish excitement. “The old woman is of no consequence,” he thought, hotly and incoherently. “The old woman was a mistake perhaps, but she is not what matters! The old woman was only an illness…. I was in a hurry to overstep…. I didn’t kill a human being, but a principle! I killed the principle, but I didn’t overstep, I stopped on this side…. I was only capable of killing. And it seems I wasn’t even capable of that… Principle? Why was that fool Razumihin abusing the socialists? They are industrious, commercial people; ‘the happiness of all’ is their case. No, life is only given to me once and I shall never have it again; I don’t want to wait for ‘the happiness of all.’ I want to live myself, or else better not live at all. I simply couldn’t pass by my mother starving, keeping my rouble in my pocket while I waited for the ‘happiness of all.’ I am putting my little brick into the happiness of all and so my heart is at peace. Ha-ha! Why have you let me slip? I only live once, I too want…. Ech, I am an æsthetic louse and nothing more,” he added suddenly, laughing like a madman. “Yes, I am certainly a louse,” he went on, clutching at the idea, gloating over it and playing with it with vindictive pleasure. “In the first place, because I can reason that I am one, and secondly, because for a month past I have been troubling benevolent Providence, calling it to witness that not for my own fleshly lusts did I undertake it, but with a grand and noble object—ha-ha! Thirdly, because I aimed at carrying it out as justly as possible, weighing, measuring and calculating. Of all the lice I picked out the most useless one and proposed to take from her only as much as I needed for the first step, no more nor less (so the rest would have gone to a monastery, according to her will, ha-ha!). And what shows that I am utterly a louse,” he added, grinding his teeth, “is that I am perhaps viler and more loathsome than the louse I killed, and I felt beforehand that I should tell myself so after killing her. Can anything be compared with the horror of that? The vulgarity! The abjectness! I understand the ‘prophet’ with his sabre, on his steed: Allah commands and ‘trembling’ creation must obey! The ‘prophet’ is right, he is right when he sets a battery across the street and blows up the innocent and the guilty without deigning to explain! It’s for you to obey, trembling creation, and not to have desires, for that’s not for you!... I shall never, never forgive the old woman!”\n",
      "\n",
      "His hair was soaked with sweat, his quivering lips were parched, his eyes were fixed on the ceiling.\n",
      "\n",
      "“Mother, sister—how I loved them! Why do I hate them now? Yes, I hate them, I feel a physical hatred for them, I can’t bear them near me…. I went up to my mother and kissed her, I remember…. To embrace her and think if she only knew… shall I tell her then? That’s just what I might do…. She must be the same as I am,” he added, straining himself to think, as it were struggling with delirium. “Ah, how I hate the old woman now! I feel I should kill her again if she came to life! Poor Lizaveta! Why did she come in?... It’s strange though, why is it I scarcely ever think of her, as though I hadn’t killed her? Lizaveta! Sonia! Poor gentle things, with gentle eyes…. Dear women! Why don’t they weep? Why don’t they moan? They give up everything… their eyes are soft and gentle…. Sonia, Sonia! Gentle Sonia!”\n",
      "\n",
      "He lost consciousness; it seemed strange to him that he didn’t remember how he got into the street. It was late evening. The twilight had fallen and the full moon was shining more and more brightly; but there was a peculiar breathlessness in the air. There were crowds of people in the street; workmen and business people were making their way home; other people had come out for a walk; there was a smell of mortar, dust and stagnant water. Raskolnikov walked along, mournful and anxious; he was distinctly aware of having come out with a purpose, of having to do something in a hurry, but what it was he had forgotten. Suddenly he stood still and saw a man standing on the other side of the street, beckoning to him. He crossed over to him, but at once the man turned and walked away with his head hanging, as though he had made no sign to him. “Stay, did he really beckon?” Raskolnikov wondered, but he tried to overtake him. When he was within ten paces he recognised him and was frightened; it was the same man with stooping shoulders in the long coat. Raskolnikov followed him at a distance; his heart was beating; they went down a turning; the man still did not look round. “Does he know I am following him?” thought Raskolnikov. The man went into the gateway of a big house. Raskolnikov hastened to the gate and looked in to see whether he would look round and sign to him. In the court-yard the man did turn round and again seemed to beckon him. Raskolnikov at once followed him into the yard, but the man was gone. He must have gone up the first staircase. Raskolnikov rushed after him. He heard slow measured steps two flights above. The staircase seemed strangely familiar. He reached the window on the first floor; the moon shone through the panes with a melancholy and mysterious light; then he reached the second floor. Bah! this is the flat where the painters were at work… but how was it he did not recognise it at once? The steps of the man above had died away. “So he must have stopped or hidden somewhere.” He reached the third storey, should he go on? There was a stillness that was dreadful…. But he went on. The sound of his own footsteps scared and frightened him. How dark it was! The man must be hiding in some corner here. Ah! the flat was standing wide open, he hesitated and went in. It was very dark and empty in the passage, as though everything had been removed; he crept on tiptoe into the parlour which was flooded with moonlight. Everything there was as before, the chairs, the looking-glass, the yellow sofa and the pictures in the frames. A huge, round, copper-red moon looked in at the windows. “It’s the moon that makes it so still, weaving some mystery,” thought Raskolnikov. He stood and waited, waited a long while, and the more silent the moonlight, the more violently his heart beat, till it was painful. And still the same hush. Suddenly he heard a momentary sharp crack like the snapping of a splinter and all was still again. A fly flew up suddenly and struck the window pane with a plaintive buzz. At that moment he noticed in the corner between the window and the little cupboard something like a cloak hanging on the wall. “Why is that cloak here?” he thought, “it wasn’t there before….” He went up to it quietly and felt that there was someone hiding behind it. He cautiously moved the cloak and saw, sitting on a chair in the corner, the old woman bent double so that he couldn’t see her face; but it was she. He stood over her. “She is afraid,” he thought. He stealthily took the axe from the noose and struck her one blow, then another on the skull. But strange to say she did not stir, as though she were made of wood. He was frightened, bent down nearer and tried to look at her; but she, too, bent her head lower. He bent right down to the ground and peeped up into her face from below, he peeped and turned cold with horror: the old woman was sitting and laughing, shaking with noiseless laughter, doing her utmost that he should not hear it. Suddenly he fancied that the door from the bedroom was opened a little and that there was laughter and whispering within. He was overcome with frenzy and he began hitting the old woman on the head with all his force, but at every blow of the axe the laughter and whispering from the bedroom grew louder and the old woman was simply shaking with mirth. He was rushing away, but the passage was full of people, the doors of the flats stood open and on the landing, on the stairs and everywhere below there were people, rows of heads, all looking, but huddled together in silence and expectation. Something gripped his heart, his legs were rooted to the spot, they would not move…. He tried to scream and woke up.\n",
      "\n",
      "He drew a deep breath—but his dream seemed strangely to persist: his door was flung open and a man whom he had never seen stood in the doorway watching him intently.\n",
      "\n",
      "Raskolnikov had hardly opened his eyes and he instantly closed them again. He lay on his back without stirring.\n",
      "\n",
      "“Is it still a dream?” he wondered and again raised his eyelids hardly perceptibly; the stranger was standing in the same place, still watching him.\n",
      "\n",
      "He stepped cautiously into the room, carefully closing the door after him, went up to the table, paused a moment, still keeping his eyes on Raskolnikov, and noiselessly seated himself on the chair by the sofa; he put his hat on the floor beside him and leaned his hands on his cane and his chin on his hands. It was evident that he was prepared to wait indefinitely. As far as Raskolnikov could make out from his stolen glances, he was a man no longer young, stout, with a full, fair, almost whitish beard.\n",
      "\n",
      "Ten minutes passed. It was still light, but beginning to get dusk. There was complete stillness in the room. Not a sound came from the stairs. Only a big fly buzzed and fluttered against the window pane. It was unbearable at last. Raskolnikov suddenly got up and sat on the sofa.\n",
      "\n",
      "“Come, tell me what you want.”\n",
      "\n",
      "“I knew you were not asleep, but only pretending,” the stranger answered oddly, laughing calmly. “Arkady Ivanovitch Svidrigailov, allow me to introduce myself….”\n"
     ]
    }
   ],
   "source": [
    "dostoyevsky_p3ch6 = open(\"dostoyevsky_p3ch6.txt\").read()\n",
    "print(dostoyevsky_p3ch6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9d9f0822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My suffering left me sad and gloomy.\n",
      "\n",
      "Academic study and the steady, mindful practice of religion slowly wrought me back to life. I have kept up\n",
      "with what some people would consider my strange religious practices. After one year of high school, I\n",
      "attended the University of Toronto and took a double-major Bachelor's degree. My majors were religious\n",
      "studies and zoology. My fourth-year thesis for religious studies concerned certain aspects of the cosmogony\n",
      "theory of Isaac Luria, the great sixteenth-century Kabbalist from Safed. My zoology thesis was a functional\n",
      "analysis of the thyroid gland of the three-toed sloth. I chose the sloth because its demeanour-calm, quiet and\n",
      "introspective-did something to soothe my shattered self.\n",
      "\n",
      "There are two-toed sloths and there are three-toed sloths, the case being determined by the forepaws of the\n",
      "animals, since all sloths have three claws on their hind paws. I had the great luck one summer of studying the\n",
      "three-toed sloth in situ in the equatorial jungles of Brazil. It is a highly intriguing creature. Its only real habit is\n",
      "indolence. It sleeps or rests on average twenty hours a day. Our team tested the sleep habits of five wild\n",
      "three-toed sloths by placing on their heads, in the early evening after they had fallen asleep, bright red plastic\n",
      "dishes filled with water. We found them still in place late the next morning, the water of the dishes swarming\n",
      "with insects. The sloth is at its busiest at sunset, using the word busy here in the most relaxed sense. It moves\n",
      "along the bough of a tree in its characteristic upside-down position at the speed of roughly 400 metres an hour.\n",
      "On the ground, it crawls to its next tree at the rate of 250 metres an hour, when motivated, which is 440 times\n",
      "slower than a motivated cheetah. Unmotivated, it covers four to five metres in an hour.\n",
      "\n",
      "The three-toed sloth is not well informed about the outside world. On a scale of 2 to 10, where 2 represents\n",
      "unusual dullness and 10 extreme acuity, Beebe (1926) gave the sloths senses of taste, touch, sight and hearing\n",
      "a rating of 2, and its sense of smell a rating of 3. If you come upon a sleeping three-toed sloth in the wild, two\n",
      "or three nudges should suffice to awaken it; it will then look sleepily in every direction but yours. Why it\n",
      "should look about is uncertain since the sloth sees everything in a Magoo-like blur. As for hearing, the sloth is\n",
      "not so much deaf as uninterested in sound. Beebe reported that firing guns next to sleeping or feeding sloths\n",
      "elicited little reaction. And the sloth's slightly better sense of smell should not be overestimated. They are said \n",
      "to be able to sniff and avoid decayed branches, but Bullock (1968) reported that sloths fall to the ground\n",
      "clinging to decayed branches \"often\".\n",
      "\n",
      "How does it survive, you might ask.\n",
      "\n",
      "Precisely by being so slow. Sleepiness and slothfulness keep it out of harm's way, away from the notice of\n",
      "jaguars, ocelots, harpy eagles and anacondas. A sloth's hairs shelter an algae that is brown during the dry\n",
      "season and green during the wet season, so the animal blends in with the surrounding moss and foliage and\n",
      "looks like a nest of white ants or of squirrels, or like nothing at all but part of a tree.\n",
      "\n",
      "The three-toed sloth lives a peaceful, vegetarian life in perfect harmony with its environment. \"A good-natured\n",
      "smile is forever on its lips,\" reported Tirler (1966). I have seen that smile with my own eyes. I am not one\n",
      "given to projecting human traits and emotions onto animals, but many a time during that month in Brazil,\n",
      "looking up at sloths in repose, I felt I was in the presence of upside-down yogis deep in meditation or hermits\n",
      "deep in prayer, wise beings whose intense imaginative lives were beyond the reach of my scientific probing.\n",
      "Sometimes I got my majors mixed up. A number of my fellow religious-studies students-muddled agnostics\n",
      "who didn't know which way was up, who were in the thrall of reason, that fool's gold for the bright-reminded\n",
      "me of the three-toed sloth; and the three-toed sloth, such a beautiful example of the miracle of life, reminded\n",
      "me of God.\n",
      "\n",
      "I never had problems with my fellow scientists. Scientists are a friendly, atheistic, hard-working, beer-drinking\n",
      "lot whose minds are preoccupied with sex, chess and baseball when they are not preoccupied with science.\n",
      "I was a very good student, if I may say so myself. I was tops at St. Michael's College four years in a row. I got\n",
      "every possible student award from the Department of Zoology. If I got none from the Department of Religious\n",
      "Studies, it is simply because there are no student awards in this department (the rewards of religious study are\n",
      "not in mortal hands, we all know that). I would have received the Governor General's Academic Medal, the\n",
      "University of Toronto's highest undergraduate award, of which no small number of illustrious Canadians have\n",
      "been recipients, were it not for a beef-eating pink boy with a neck like a tree trunk and a temperament of\n",
      "unbearable good cheer.\n",
      "\n",
      "I still smart a little at the slight. When you've suffered a great deal in life, each additional pain is both\n",
      "unbearable and trifling. My life is like a memento mori painting from European art: there is always a grinning\n",
      "skull at my side to remind me of the folly of human ambition. I mock this skull. I look at it and I say, \"You've\n",
      "got the wrong fellow. You may not believe in life, but I don't believe in death. Move on!\" The skull snickers\n",
      "and moves ever closer, but that doesn't surprise me. The reason death sticks so closely to life isn't biological\n",
      "necessity-it's envy. Life is so beautiful that death has fallen in love with it, a jealous, possessive love that grabs\n",
      "at what it can. But life leaps over oblivion lightly, losing only a thing or two of no importance, and gloom is\n",
      "but the passing shadow of a cloud. The pink boy also got the nod from the Rhodes Scholarship committee. I\n",
      "love him and I hope his time at Oxford was a rich experience. If Lakshmi, goddess of wealth, one day favours\n",
      "me bountifully, Oxford is fifth on the list of cities I would like to visit before I pass on, after Mecca, Varanasi,\n",
      "Jerusalem and Paris.\n",
      "\n",
      "I have nothing to say of my working life, only that a tie is a noose, and inverted though it is, it will hang a man nonetheless if he's not careful.\n",
      "\n",
      "I love Canada. I miss the heat of India, the food, the house lizards on the walls, the musicals on the silver\n",
      "screen, the cows wandering the streets, the crows cawing, even the talk of cricket matches, but I love Canada.\n",
      "It is a great country much too cold for good sense, inhabited by compassionate, intelligent people with bad\n",
      "hairdos. Anyway, I have nothing to go home to in Pondicherry.\n",
      "\n",
      "Richard Parker has stayed with me. I've never forgotten him. Dare I say I miss him? I do. I miss him. I still see\n",
      "him in my dreams. They are nightmares mostly, but nightmares tinged with love. Such is the strangeness of\n",
      "the human heart. I still cannot understand how he could abandon me so unceremoniously, without any sort of\n",
      "goodbye, without looking back even once. That pain is like an axe that chops at my heart.\n",
      "The doctors and nurses at the hospital in Mexico were incredibly kind to me. And the patients, too. Victims of\n",
      "cancer or car accidents, once they heard my story, they hobbled and wheeled over to see me, they and their\n",
      "families, though none of them spoke English and I spoke no Spanish. They smiled at me, shook my hand,\n",
      "patted me on the head, left gifts of food and clothing on my bed. They moved me to uncontrollable fits of\n",
      "laughing and crying.\n",
      "\n",
      "Within a couple of days I could stand, even make two, three steps, despite nausea, dizziness and general\n",
      "weakness. Blood tests revealed that I was anemic, and that my level of sodium was very high and my\n",
      "potassium low. My body retained fluids and my legs swelled up tremendously. I looked as if I had been\n",
      "grafted with a pair of elephant legs. My urine was a deep, dark yellow going on to brown. After a week or so, I\n",
      "could walk just about normally and I could wear shoes if I didn't lace them up. My skin healed, though I still\n",
      "have scars on my shoulders and back.\n",
      "\n",
      "The first time I turned a tap on, its noisy, wasteful, superabundant gush was such a shock that I became\n",
      "incoherent and my legs collapsed beneath me and I fainted in the arms of a nurse.\n",
      "The first time I went to an Indian restaurant in Canada I used my fingers. The waiter looked at me critically\n",
      "and said, \"Fresh off the boat, are you?\" I blanched. My fingers, which a second before had been taste buds\n",
      "savouring the food a little ahead of my mouth, became dirty under his gaze. They froze like criminals caught\n",
      "in the act. I didn't dare lick them. I wiped them guiltily on my napkin. He had no idea how deeply those words\n",
      "wounded me. They were like nails being driven into my flesh. I picked up the knife and fork. I had hardly ever\n",
      "used such instruments. My hands trembled. My sambar lost its taste.\n"
     ]
    }
   ],
   "source": [
    "martel_ch1 = open(\"martel_ch1.txt\").read()\n",
    "print(martel_ch1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc6b25",
   "metadata": {},
   "source": [
    "Shortened versions from each of the chapters above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08c8672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old woman was a mistake perhaps, but she’s not the point! The old woman was merely a sickness . . . I was in a hurry to step over . . . it wasn’t a human being I killed, it was a principle! So I killed the principle, but I didn’t step over, I stayed on this side . . . All I managed to do was kill. And I didn’t even manage that, as it turns out . . .\n"
     ]
    }
   ],
   "source": [
    "dostoyevsky_excerpt = \"The old woman was a mistake perhaps, but she’s not the point! The old woman was merely a sickness . . . I was in a hurry to step over . . . it wasn’t a human being I killed, it was a principle! So I killed the principle, but I didn’t step over, I stayed on this side . . . All I managed to do was kill. And I didn’t even manage that, as it turns out . . .\"\n",
    "print(dostoyevsky_excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "1dcf42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard Parker has stayed with me. I've never forgotten him. Dare I say I miss him? I do. I miss him. I still see him in my dreams. They are nightmares mostly, but nightmares tinged with love. Such is the strangeness of the human heart. I still cannot understand how he could abandon me so unceremoniously, without any sort of goodbye, without looking back even once. The pain is like an axe that chops my heart.\n"
     ]
    }
   ],
   "source": [
    "martel_excerpt = \"Richard Parker has stayed with me. I've never forgotten him. Dare I say I miss him? I do. I miss him. I still see him in my dreams. They are nightmares mostly, but nightmares tinged with love. Such is the strangeness of the human heart. I still cannot understand how he could abandon me so unceremoniously, without any sort of goodbye, without looking back even once. The pain is like an axe that chops my heart.\"\n",
    "print(martel_excerpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d463cc8",
   "metadata": {},
   "source": [
    "## Markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448a7cf",
   "metadata": {},
   "source": [
    "Install Markovify + import library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ef7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install markovify\n",
    "\n",
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655ce9b",
   "metadata": {},
   "source": [
    "Create generators using excerpts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54046157",
   "metadata": {},
   "outputs": [],
   "source": [
    "dostoyevsky_short_generator = markovify.Text(dostoyevsky_excerpt)\n",
    "martel_short_generator = markovify.Text(martel_excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "11148b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old woman was merely a sickness . . I was in a hurry to step over . . I was in a hurry to step over . . . I was in a hurry to step over . .\n"
     ]
    }
   ],
   "source": [
    "print(dostoyevsky_short_generator.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d8867",
   "metadata": {},
   "source": [
    "I kind of like the repetitiveness in this one? It's still too similar to the source text though, so I'm trying this with a different state size. I think the ellipses are throwing it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2209bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So I killed the point!\n"
     ]
    }
   ],
   "source": [
    "dostoyevsky_short_generator_2 = markovify.Text(dostoyevsky_excerpt, state_size=1)\n",
    "print(dostoyevsky_short_generator_2.make_sentence(test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1fbfd2",
   "metadata": {},
   "source": [
    "Still too similar for my liking. I'm going to try breaking it down further by character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6ca85f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesByChar(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914ac55",
   "metadata": {},
   "source": [
    "Dostoevsky test using short and long texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8deb864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So I kill.\n"
     ]
    }
   ],
   "source": [
    "dostoyevsky_gen_char = SentencesByChar(dostoyevsky_excerpt, state_size=4)\n",
    "print(dostoyevsky_gen_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "89af1a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fly flew by and the sofa and how annoying in from the door was soaked with shaking along while, and a wretched skinny old woman.\n"
     ]
    }
   ],
   "source": [
    "dostoyevsky_full_gen_char = SentencesByChar(dostoyevsky_p3ch6, state_size=5)\n",
    "print(dostoyevsky_full_gen_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de976b",
   "metadata": {},
   "source": [
    "Martel test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "402d849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such is that chops my dreams.\n"
     ]
    }
   ],
   "source": [
    "martel_gen_char = SentencesByChar(martel_excerpt, state_size=3)\n",
    "print(martel_gen_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8971c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dare I say I miss the house lizards on their hind paws.\n"
     ]
    }
   ],
   "source": [
    "martel_full_gen_char = SentencesByChar(martel_ch1, state_size=6)\n",
    "print(martel_full_gen_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4cf40",
   "metadata": {},
   "source": [
    "Seems that the full chapters are working better than my short versions, so I'm going to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5a89fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dostoyevsky_full_gen = markovify.Text(dostoyevsky_p3ch6)\n",
    "martel_full_gen = markovify.Text(martel_ch1)\n",
    "combo = markovify.combine([dostoyevsky_full_gen, martel_full_gen], [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5e0a284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My zoology thesis was a peculiar breathlessness in the long coat.\n"
     ]
    }
   ],
   "source": [
    "print(combo.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca4209",
   "metadata": {},
   "source": [
    "More output(s):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a025a",
   "metadata": {},
   "source": [
    "~~~\n",
    "When you've suffered a great deal in life, but I don't believe in life, each additional pain is like an axe that chops at my side to remind me of God.\n",
    "~~~\n",
    "\n",
    "~~~\n",
    "I killed the principle, but I love Canada.\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c6f60",
   "metadata": {},
   "source": [
    "Switching between words and characters to see if that makes a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "bf6dc23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not a sound came from the Department of Zoology.\n",
      "That pain is both unbearable and trifling.\n"
     ]
    }
   ],
   "source": [
    "level = \"word\"\n",
    "\n",
    "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
    "gen_a = model_cls(dostoyevsky_p3ch6, state_size=2)\n",
    "gen_b = model_cls(martel_ch1, state_size=2)\n",
    "gen_combo = markovify.combine([gen_a, gen_b], [0.5, 0.5])\n",
    "for i in range(2):\n",
    "    out = gen_combo.make_short_sentence(280, test_output=False)\n",
    "    out = out.replace(\"\\n\", \" \")\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945d9c7",
   "metadata": {},
   "source": [
    "More output(s):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffdb27",
   "metadata": {},
   "source": [
    "~~~\n",
    "It’s too inartistic.\n",
    "And perhaps he was he, what it was!\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a7525",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb496427",
   "metadata": {},
   "source": [
    "Import library and install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5e64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!conda install --prefix {sys.prefix} -y -c pytorch pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97565692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "51ae292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3bcfb067",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d8a5d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "957abe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea16b9",
   "metadata": {},
   "source": [
    "Other stuff I need to add to train the model. I'll use the Dostoyevsky text for this and then attach it to the Martel text after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f0c69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5b0b1-d556-4f9c-b590-963b16019e90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13296b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9e72dd02-6cd2-44c6-b4ea-562ed5725003",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.load_dataset('text', data_files=\"dostoyevsky_p3ch6.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a18ffa-a382-4754-90a6-3c85cbce1b93",
   "metadata": {},
   "source": [
    " Nevermind! This part keeps giving me an error and I can't figure out why. I think the updated version of the Hugging Face Transformers library has some differences in how it's used. I tried different codes but it kept giving me a new error after fixing the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ae0bbb8d-0044-42f3-b8fc-5ffce56ef15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_training_data = training_data.map(\n",
    "    lambda x: tokenizer(x['text']),\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "925f006f-bfe1-49f6-833b-ad16c7a069cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "# magic from https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_training_data = tokenized_training_data.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6026a55c-569a-4187-bdd2-3653a23387bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882a00d-1c4a-4610-867b-afc2b6947e11",
   "metadata": {},
   "source": [
    "Error message told me to run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb9834-15ff-4aa6-a654-7f9268934c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3ad2391b-d6c2-4f77-8d4e-a3de931dd132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "\n",
    "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
    "accelerator = Accelerator(dataloader_config=dataloader_config)\n",
    "\n",
    "model = model.to(accelerator.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d65530-471e-4f53-8915-a40cb742dac4",
   "metadata": {},
   "source": [
    "That seemed like it worked at first but... still no!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e627cc38-dc3f-4ee4-a307-4a8364d2630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  train_dataset=lm_training_data['train'],\n",
    "                  args=TrainingArguments(\n",
    "                      output_dir='distilgpt2-finetune-dostoyevsky',\n",
    "                      num_train_epochs=1,\n",
    "                      do_train=True,\n",
    "                      do_eval=False\n",
    "                  ),\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b1001f2-fb7b-4607-ad1e-2faf823351a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=3.7863972981770835, metrics={'train_runtime': 2.0589, 'train_samples_per_second': 44.197, 'train_steps_per_second': 5.828, 'total_flos': 1486125268992.0, 'train_loss': 3.7863972981770835, 'epoch': 1.0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "21723911-17f9-48fc-9d69-34937f90c393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d76af03a-7339-4485-8fc8-9daedc7d0384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTwo roads diverged in a yellow\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:219\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1156\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1169\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1168\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1169\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1170\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1067\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1068\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1069\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:295\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[1;32m    296\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1526\u001b[0m         input_ids,\n\u001b[1;32m   1527\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1528\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[1;32m   1529\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1530\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1531\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m   1532\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1533\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1534\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1535\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1537\u001b[0m     )\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2622\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2624\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2625\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2626\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2627\u001b[0m )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m   1075\u001b[0m     input_ids,\n\u001b[1;32m   1076\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1077\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1079\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1080\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1082\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1084\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1085\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1087\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:837\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    834\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layer)\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwte(input_ids)\n\u001b[1;32m    838\u001b[0m position_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "generator(\"Two roads diverged in a yellow\", max_length=100)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0521b26-045c-4fa2-a0c5-136fabe13ffe",
   "metadata": {},
   "source": [
    "***\n",
    "**It's not working out and I've given up for now, so back to using the model as is.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8e7bdd41-52a7-406d-9aad-e4f6101b22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfe703-066c-4f73-b423-53f17c5c81d5",
   "metadata": {},
   "source": [
    "Tried to make the red warnings go away when I run the code below this to print but it did not work :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a47ed-5b68-4dff-b6e6-51c304553b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shutup\n",
    "import shutup;\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "be451f30-ff80-4660-ad6a-cfc361a17248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wasn’t a human being I killed, it was a human being I killed, it was a human being I killed, it was a human being I killed, it was a human being \n",
      "I've never forgotten that the world of the game is a very different place than the world of the game. I've never forgotten that the world \n",
      "The pain is like a broken bone\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wasn’t a human being I killed, it was a living being. As the story is, humanhood's human essence exists, and the human being cannot escape within as long as human \n",
      "I've never forgotten that it was during that period when I was at BYU, as I would never have imagined it had not existed in the early \n",
      "The pain is like getting punched one\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wasn’t a human being I killed, it was a human being and no. Just like this is an age. It feels lonely in that I really think we have that sorts and \n",
      "I've never forgotten who those little bastard had was from what you went here over on Friday for lunch when he had us picket in the \n",
      "The pain is like the body tearing\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wasn’t a human being I killed, it was a war that will destroy our civilizations that had made my culture disappear as many civilizations and civilization that took our peoples life into chaos until war \n",
      "I've never forgotten about any type of group identity – not so for as far out. Many, on occasion -- think on whether or even some \n",
      "The pain is like soapy,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_count = 0.2\n",
    "\n",
    "for i in range(4):\n",
    "    print(generator(\"It wasn’t a human being I killed, it was a\", bad_words_ids=tokenizer([\"\\n\"]).input_ids, temperature=temp_count, max_length=40)[0]['generated_text'], \n",
    "          generator(\"\\nI've never forgotten\", temperature=temp_count, max_length=30)[0]['generated_text'], \n",
    "          generator(\"\\nThe pain is like\", temperature=temp_count, max_length=8)[0]['generated_text'])\n",
    "    print('')\n",
    "    temp_count += 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237a33-44fe-4815-a5bb-91c138978743",
   "metadata": {},
   "source": [
    "More output(s):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8ac46-f0ab-4f24-8df6-fdc830f1473a",
   "metadata": {},
   "source": [
    "~~~\n",
    "It wasn’t a human being I killed, it was a human being I killed, it was a human being I killed, it was a human being I killed, it was a human being \n",
    "I've never forgotten that the world is a place where people are not allowed to be in a place where they can be in a place where they \n",
    "The pain is like a broken bone\n",
    "\n",
    "It wasn’t a human being I killed, it was a human being. ",
    "․ It’ was a human being who created the world just as human beings are. It \n",
    "I've never forgotten to look back to my life. The day started by watching the TV for a minute or two, then I realized it was \n",
    "The pain is like a broken toe\n",
    "\n",
    "It wasn’t a human being I killed, it was a family of children.\" said the court proceedings to the man during one hour with little else; a few, with the blessing, walked \n",
    "I've never forgotten just something else — how well it is possible every single act you may've made has one that brings on a life-saving \n",
    "The pain is like falling down the\n",
    "\n",
    "It wasn’t a human being I killed, it was a boy named Saki and Heiki †I went along who lived to eat.’\"I did it every two steps \n",
    "I've never forgotten an election in Ireland for whom I think there was something that we shouldn's want. People might think like they see politicians go \n",
    "The pain is like that, this\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56332f-f69d-4e03-b29f-8ecd52b878d9",
   "metadata": {},
   "source": [
    "Keeping the beginning and end of the original excerpt and only replacing the middle with generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "29345014-9069-4fd0-af34-b5ad2af88894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wasn’t a human being I killed, it was a monster we'd never see.It didn’t happen. But it wasn’t a woman\n",
      "All I managed to do was kill. And I didn’t even manage that, as it turns out . . .\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"It wasn’t a human being I killed, it was a\", bad_words_ids=tokenizer([\"\\n\"]).input_ids, max_length=35)[0]['generated_text'])\n",
    "print(\"All I managed to do was kill. And I didn’t even manage that, as it turns out . . .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9e2d65a1-7e7b-4128-88f1-af1068b029db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard Parker has stayed with me. I've never forgotten him. We both got on top of my heart and then at the end of my life,\n",
      "I still see him in my dreams.\n"
     ]
    }
   ],
   "source": [
    "print(generator(\"Richard Parker has stayed with me. I've never forgotten him.\", bad_words_ids=tokenizer([\"\\n\"]).input_ids, temperature=1.5, max_length=30)[0]['generated_text'])\n",
    "print(\"I still see him in my dreams.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
